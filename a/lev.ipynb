{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  model.zip\n",
      "  inflating: model.pt                \n"
     ]
    }
   ],
   "source": [
    "!unzip model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt >> none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.23.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lyTAfsc6PwYV"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from madgrad import MADGRAD, MirrorMADGRAD\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    get_constant_schedule,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_polynomial_decay_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pSMMo28UPija"
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"/content/drive/MyDrive/ml/Контесты/rucode_6/a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avpm7R4pykVC"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Db9Othg-yj3p"
   },
   "outputs": [],
   "source": [
    "classes = ['Red', 'Green', 'Violet', 'White', 'Yellow', 'Brown', 'Black', 'Blue', 'Cyan', 'Grey', 'Orange']\n",
    "counts = []\n",
    "for class_name in classes:\n",
    "    counts.append(len(os.listdir(f\"./data_ext/train/{class_name}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NKYJqqDcCeeM",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 11 artists>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(classes, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-l0SNG0kl6"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MrSjnBd20mSk"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight, gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, s=45.0, m=0.1, crit=\"bce\", weight=None, reduction=\"mean\",\n",
    "                 focal_loss_gamma=0, class_weights_norm=\"batch\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.class_weights_norm = class_weights_norm\n",
    "        \n",
    "        if crit == \"focal\":\n",
    "            self.crit = FocalLoss(gamma=focal_loss_gamma)\n",
    "        elif crit == \"bce\":\n",
    "            self.crit = nn.CrossEntropyLoss(reduction=\"none\")   \n",
    "\n",
    "        if s is None:\n",
    "            self.s = torch.nn.Parameter(torch.tensor([45.], requires_grad=True, device='cuda'))\n",
    "        else:\n",
    "            self.s = s\n",
    "\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        \n",
    "        labels2 = torch.zeros_like(cosine)\n",
    "        labels2.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        labels2 *= (1 - 0.1)\n",
    "        labels2 += 0.005\n",
    "        output = (labels2 * phi) + ((1.0 - labels2) * cosine)\n",
    "\n",
    "        s = self.s\n",
    "\n",
    "        output = output * s\n",
    "        loss = self.crit(output, labels)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            w = self.weight[labels].to(logits.device)\n",
    "\n",
    "            loss = loss * w\n",
    "            if self.class_weights_norm == \"batch\":\n",
    "                loss = loss.sum() / w.sum()\n",
    "            if self.class_weights_norm == \"global\":\n",
    "                loss = loss.mean()\n",
    "            else:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            return loss\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "girCXnzE0m48"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A_8SSmFU0nfO"
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B3NCJxD1PeS"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jOkK463m1Qbk"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, backbone, output_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        del backbone.fc\n",
    "        self.backbone.dropout = nn.Dropout(p=0.1)\n",
    "        self.backbone.fc = torch.nn.Linear(in_features=2048, out_features=output_dim, bias=True)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        out = self.backbone(image)\n",
    "#         out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7DhepxS1dO_"
   },
   "source": [
    "# Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "016aad72"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_function, optimizer, scheduler, device, n_accumulated_grads=0):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    dl_size = len(data_loader)\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    batch_i = 0\n",
    "    steps_to_accumulate_grads = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        image, target = batch\n",
    "        image = image[\"pixel_values\"][0].to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        logits = model(image).logits\n",
    "                \n",
    "        preds.append(logits.argmax(dim=1))\n",
    "        targets.append(target)\n",
    "                \n",
    "        loss = loss_function(logits, target)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    acc = (targets == preds).sum() / preds.shape[0]\n",
    "    f1 = f1_score(preds.cpu(), targets.cpu(), average='macro')\n",
    "    \n",
    "    metrics = {\n",
    "        \"Train Loss\": total_train_loss / dl_size,\n",
    "        \"Train Accuracy\": acc.item(),\n",
    "        \"Train F1\": f1.item()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "    \n",
    "def eval_epoch(model, data_loader, loss_function, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    dl_size = len(data_loader)\n",
    "\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        image, target = batch\n",
    "        image = image[\"pixel_values\"][0].to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(image).logits\n",
    "            preds.append(logits.argmax(dim=1))\n",
    "            targets.append(target)\n",
    "        \n",
    "        loss = loss_function(logits, target)\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    acc = (targets == preds).sum() / preds.shape[0]\n",
    "    f1 = f1_score(preds.cpu(), targets.cpu(), average='macro')\n",
    "    \n",
    "    metrics = {\n",
    "        \"Eval Loss\": total_train_loss / dl_size,\n",
    "        \"Eval Accuracy\": acc.item(),\n",
    "        \"Eval F1\": f1.item()\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model, path+'/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "602cd4af"
   },
   "outputs": [],
   "source": [
    "def cross_validation(project_name,\n",
    "                     model, \n",
    "                     dataset, \n",
    "                     loss_function, \n",
    "                     strat_array=None,\n",
    "                     device=torch.device(\"cuda\"),\n",
    "                     random_state: int=69, \n",
    "                     shuffle: bool=True, \n",
    "                     n_folds: int=4, \n",
    "                     epochs: int=5, \n",
    "                     lr: float=1e-6,\n",
    "                     start_fold: int=0, \n",
    "                     batch_size: int=32,\n",
    "                     iters_to_accumulate=None,\n",
    "                     n_accumulated_grads: int = 0):\n",
    "    random.seed(random_state),\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    \n",
    "    loss_function.to(device)\n",
    "    if strat_array:\n",
    "        kfold = StratifiedKFold(n_folds, shuffle=shuffle, random_state=random_state)\n",
    "        split = kfold.split(dataset, strat_array)\n",
    "    else: \n",
    "        kfold = KFold(n_folds, shuffle=shuffle, random_state=random_state)\n",
    "        split = kfold.split(dataset)\n",
    "\n",
    "    for fold, (train_ids, eval_ids) in enumerate(split):\n",
    "        if fold >= start_fold:\n",
    "            print(f'FOLD {fold}')\n",
    "            print('--------------------------------')\n",
    "            \n",
    "            for p in model.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "                else:\n",
    "                    nn.init.normal_(p)\n",
    "            \n",
    "            '''run = wandb.init(\n",
    "                name=f\"fold_{fold}\",\n",
    "                project=f\"{project_name}_fold_{fold}\",\n",
    "                config={ \n",
    "                         \"random_state\": random_state, \n",
    "                         \"shuffle\": shuffle,\n",
    "                         \"epochs\": epochs, \n",
    "                         \"learning_rate\": lr,\n",
    "                         \"batch_size\": batch_size,\n",
    "                         \"iters_to_accumulate\": iters_to_accumulate\n",
    "                        }\n",
    "            )'''\n",
    "\n",
    "            optimizer = MADGRAD(\n",
    "            model.parameters(),\n",
    "            lr = lr, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "        )\n",
    "\n",
    "            train_subsampler = torch.utils.data.Subset(dataset,  train_ids)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                          train_subsampler, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle)\n",
    "\n",
    "            eval_subsampler = torch.utils.data.Subset(dataset,  eval_ids)\n",
    "            eval_loader = torch.utils.data.DataLoader(\n",
    "                          eval_subsampler,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle)\n",
    "            \n",
    "            total_steps = len(train_loader) * epochs \n",
    "\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "            mrrs = []\n",
    "\n",
    "            for epoch_i in range(0, epochs):\n",
    "                train_metrics = train_epoch(model, train_loader, loss_function, optimizer, scheduler, device)\n",
    "                eval_metrics = eval_epoch(model, eval_loader, loss_function, device)\n",
    "                \n",
    "                print(f\"EPOCH: {epoch_i}\")\n",
    "                print(train_metrics)\n",
    "                print(eval_metrics)\n",
    "                \n",
    "                #run.log(train_metrics)\n",
    "                #run.log(eval_metrics)\n",
    "                            \n",
    "            #run.finish()\n",
    "\n",
    "\n",
    "def single_model(model, \n",
    "                     dataset, \n",
    "                     loss_function, \n",
    "                     device=torch.device(\"cuda\"),\n",
    "                     random_state: int=69, \n",
    "                     shuffle=True,\n",
    "                     epochs: int=15, \n",
    "                     lr: float=1e-6,\n",
    "                     batch_size: int=32,\n",
    "                     start_epoch=0\n",
    "                     ):\n",
    "    random.seed(random_state),\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "    \n",
    "    loss_function.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = MADGRAD(\n",
    "        model.parameters(),\n",
    "        lr = lr, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    )\n",
    "    \n",
    "    fold_path = f'vit'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.mkdir(fold_path)\n",
    "        \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    total_steps = len(data_loader) * epochs \n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        if epoch_i >= start_epoch:\n",
    "            train_metrics = train_epoch(model, data_loader, loss_function, optimizer, scheduler, device)\n",
    "            epoch_path = fold_path+f'/epoch_{epoch_i + 9}'\n",
    "            if not os.path.exists(epoch_path):\n",
    "                os.mkdir(epoch_path)\n",
    "            save_model(model, epoch_path)\n",
    "            print(\"EPOCH\", epoch_i)\n",
    "            print(train_metrics)\n",
    "            # eval_epoch(fold_model, eval_loader, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(model, dataset, device):\n",
    "    score = np.zeros(11)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = True)\n",
    "    for image,target in tqdm(data_loader):\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "            if preds.argmax(dim = 1).item() == target.item():\n",
    "                score[preds.argmax(dim = 1).item()] += 1\n",
    "    \n",
    "    #for i,fold in enumerate(os.listdir('data_ext/train')):\n",
    "        #score[i] /= len(os.listdir(f'data_ext/train/{fold}'))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_confusion_matrix(model, dataset, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = False)\n",
    "    \n",
    "    for image, target in tqdm(data_loader):\n",
    "        image = image.to(device)\n",
    "        taget = target.to(device)\n",
    "        targets.append(target.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(image)\n",
    "            preds.append(logits.argmax(dim = 1).item())\n",
    "    \n",
    "    conf_mat = confusion_matrix(targets, preds, labels = [x for x in range(11)])\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOY06p6r2Ur9"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 20 23:49:05 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA Graphics...  On   | 00000000:05:00.0  On |                  Off |\r\n",
      "|  0%   42C    P8    34W / 450W |   5761MiB / 24564MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2113      G   /usr/lib/xorg/Xorg                121MiB |\r\n",
      "|    0   N/A  N/A      2433      G   /usr/bin/gnome-shell               41MiB |\r\n",
      "|    0   N/A  N/A      4627      G   ...7/usr/lib/firefox/firefox      141MiB |\r\n",
      "|    0   N/A  N/A    442436      G   telegram-desktop                   10MiB |\r\n",
      "|    0   N/A  N/A    482032      C   /opt/conda/bin/python            5442MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 1599.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown\n",
      "Orange\n",
      "Violet\n",
      "Black\n",
      "Yellow\n",
      "Grey\n",
      "Red\n",
      "White\n",
      "Cyan\n",
      "Blue\n",
      "Green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strat_array = []\n",
    "target_class = 0\n",
    "for folder in tqdm(os.listdir('data_ext/train')):\n",
    "    for _ in range(len(os.listdir('data_ext/train'+f'/{folder}'))):\n",
    "        strat_array.append(target_class)\n",
    "    print(folder)\n",
    "    target_class += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(strat_array)\n",
    "weights = []\n",
    "classes = np.arange(11)\n",
    "correct_weights = compute_class_weight(\n",
    "                                        class_weight = 'balanced',\n",
    "                                        classes = classes,\n",
    "                                        y = np.array(strat_array) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09147905, 0.79192226, 1.12558777, 1.06586271, 0.8263809 ,\n",
       "       1.10651002, 0.72790624, 1.05937673, 3.91215526, 0.77719156,\n",
       "       0.97987378])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model_weights = torch.load('model.pt')\n",
    "# model = torchvision.models.resnet50(weights=model_weights)\n",
    "# model.fc = nn.Linear(2048, 11)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"therealcyberlord/stanford-car-vit-patch16\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"therealcyberlord/stanford-car-vit-patch16\")\n",
    "model.classifier = nn.Linear(768, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=FocalLoss(weight=torch.tensor(correct_weights).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black',\n",
       " 'Blue',\n",
       " 'Brown',\n",
       " 'Cyan',\n",
       " 'Green',\n",
       " 'Grey',\n",
       " 'Orange',\n",
       " 'Red',\n",
       " 'Violet',\n",
       " 'White',\n",
       " 'Yellow']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transforms=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(45),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.6),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.6),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(size=(224,224)),\n",
    "#     torchvision.transforms.Lambda(lambda a: a / 255),\n",
    "    torchvision.transforms.Lambda(lambda a: extractor(a)),\n",
    "\n",
    "    \n",
    "#     torchvision.transforms.Normalize(mean, std)\n",
    "])\n",
    "train_dataset = torchvision.datasets.ImageFolder(f\"./data_ext/train\", transform=transforms)\n",
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "5z7MLDul2V7J",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 539/539 [01:03<00:00,  8.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████| 180/180 [00:12<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "{'Train Loss': 1.9600689103740487, 'Train Accuracy': 0.2808401882648468, 'Train F1': 0.2435291673441788}\n",
      "{'Eval Loss': 1.2757467092739212, 'Eval Accuracy': 0.5040028095245361, 'Eval F1': 0.3991138084929604}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████▏                  | 375/539 [00:44<00:19,  8.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcar_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mstrat_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrat_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m69\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mstart_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [118], line 79\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(project_name, model, dataset, loss_function, strat_array, device, random_state, shuffle, n_folds, epochs, lr, start_fold, batch_size, iters_to_accumulate, n_accumulated_grads)\u001b[0m\n\u001b[1;32m     76\u001b[0m mrrs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[0;32m---> 79\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     eval_metrics \u001b[38;5;241m=\u001b[39m eval_epoch(model, eval_loader, loss_function, device)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [113], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_function, optimizer, scheduler, device, n_accumulated_grads)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/madgrad/madgrad.py:187\u001b[0m, in \u001b[0;36mMADGRAD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(x0\u001b[38;5;241m.\u001b[39maddcdiv(s, rms, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# p is a moving average of z\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ck)\u001b[38;5;241m.\u001b[39madd_(z, alpha\u001b[38;5;241m=\u001b[39mck)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation(project_name=\"car_classification\",\n",
    "                     model=model, \n",
    "                     dataset=train_dataset,\n",
    "                     strat_array=strat_array,\n",
    "                     loss_function=loss_function, \n",
    "                     device=torch.device(\"cuda\"),\n",
    "                     random_state=69, \n",
    "                     n_folds=4, \n",
    "                     epochs=60, \n",
    "                     lr=1e-5,\n",
    "                     start_fold=0, \n",
    "                     batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"therealcyberlord/stanford-car-vit-patch16\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"therealcyberlord/stanford-car-vit-patch16\")\n",
    "model.classifier = nn.Linear(768, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"vit/epoch_9/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 719/719 [01:24<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "{'Train Loss': 0.034411304865750274, 'Train Accuracy': 0.987641453742981, 'Train F1': 0.9878719419248447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 719/719 [01:24<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "{'Train Loss': 0.021765308085764982, 'Train Accuracy': 0.9916449189186096, 'Train F1': 0.9919398162485961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 719/719 [01:24<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2\n",
      "{'Train Loss': 0.024446832974720365, 'Train Accuracy': 0.9915578961372375, 'Train F1': 0.9920070136956093}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 719/719 [01:25<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3\n",
      "{'Train Loss': 0.01959475742733149, 'Train Accuracy': 0.9944299459457397, 'Train F1': 0.9951168999185307}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 719/719 [01:27<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4\n",
      "{'Train Loss': 0.015836114311266844, 'Train Accuracy': 0.9943429231643677, 'Train F1': 0.9948136380550111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████▋                                               | 170/719 [00:20<01:05,  8.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [187], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msingle_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m69\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [185], line 134\u001b[0m, in \u001b[0;36msingle_model\u001b[0;34m(model, dataset, loss_function, device, random_state, shuffle, epochs, lr, batch_size, start_epoch)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_epoch:\n\u001b[0;32m--> 134\u001b[0m         train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         epoch_path \u001b[38;5;241m=\u001b[39m fold_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m9\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(epoch_path):\n",
      "Cell \u001b[0;32mIn [113], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_function, optimizer, scheduler, device, n_accumulated_grads)\u001b[0m\n\u001b[1;32m     11\u001b[0m batch_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m steps_to_accumulate_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     14\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     15\u001b[0m     image \u001b[38;5;241m=\u001b[39m image[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/PIL/Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    875\u001b[0m ):\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/rucode_6/a/venv/lib/python3.10/site-packages/PIL/ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "single_model(model = model, \n",
    "             dataset = train_dataset, \n",
    "             loss_function = loss_function, \n",
    "             device = torch.device(\"cuda\"),\n",
    "             random_state = 69, \n",
    "             shuffle = True,\n",
    "             epochs = 15, \n",
    "             lr = 1e-5,\n",
    "             batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"vit/epoch_12/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms=torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.RandomRotation(45),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.6),\n",
    "#     torchvision.transforms.RandomVerticalFlip(p=0.6),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(size=(224,224)),\n",
    "#     torchvision.transforms.Lambda(lambda a: a / 255),\n",
    "    torchvision.transforms.Lambda(lambda a: extractor(a)),\n",
    "\n",
    "    \n",
    "#     torchvision.transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, root=f\"./data/public_test/\", transform=transforms):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    img_files = sorted(os.listdir(root), key=lambda x: int(x.split(\".\")[0]))\n",
    "    for img_file in tqdm(img_files):\n",
    "        img_rgb = Image.open(root + img_file)\n",
    "        image = test_transforms(img_rgb)\n",
    "        image = torch.tensor(image[\"pixel_values\"][0]).unsqueeze(dim=0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(image).logits.argmax(dim=1).cpu().item()\n",
    "            preds.append(pred)\n",
    "            \n",
    "    return pd.Series(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1436/1436 [00:09<00:00, 156.44it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, device=torch.device(\"cuda\"), root=f\"data_ext/public_test/\", transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.apply(lambda x: train_dataset.classes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv(\"vit_submission_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11490/11490 [01:33<00:00, 122.48it/s]\n"
     ]
    }
   ],
   "source": [
    "anal = analyze(model = model, dataset=train_dataset, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "counts = []\n",
    "for class_name in classes:\n",
    "    counts.append(len(os.listdir(f\"./data_ext/train/{class_name}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    anal[i] /= counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.9968652 , 1.        , 0.99906191,\n",
       "       1.        , 0.9969674 , 0.99721254, 1.        , 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11490/11490 [01:33<00:00, 123.32it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = build_confusion_matrix(model = model, dataset = train_dataset, device = 'cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 979,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0],\n",
       "       [   0, 1344,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   1,    0,  952,    0,    0,    1,    0,    0,    0,    0,    3],\n",
       "       [   0,    0,    0,  267,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1065,    0,    0,    0,    0,    0,    1],\n",
       "       [   0,    0,    0,    0,    0,  944,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0, 1317,    0,    0,    0,    2],\n",
       "       [   0,    1,    0,    0,    0,    0,    1, 1433,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,  928,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,  986,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1264]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black',\n",
       " 'Blue',\n",
       " 'Brown',\n",
       " 'Cyan',\n",
       " 'Green',\n",
       " 'Grey',\n",
       " 'Orange',\n",
       " 'Red',\n",
       " 'Violet',\n",
       " 'White',\n",
       " 'Yellow']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'ABC'\n",
    "ct = 0\n",
    "for a in st:\n",
    "    for b in st:\n",
    "        for c in st:\n",
    "            s = a+b+c\n",
    "            if 'AA' not in s:\n",
    "                ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNegCHDBaahl+Ne/cBGwqh0",
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
